{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings \n",
    "\n",
    "from sklearn.metrics import mean_squared_error,mean_squared_log_error\n",
    "from sklearn.model_selection import train_test_split,KFold,StratifiedKFold,GridSearchCV,RandomizedSearchCV,cross_val_score,RepeatedKFold\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler,Normalizer,RobustScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import sklearn.ensemble as ensemble\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.ensemble import RandomForestClassifier,BaggingClassifier,AdaBoostClassifier,GradientBoostingClassifier,RandomForestRegressor,BaggingRegressor,AdaBoostRegressor,GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression,LogisticRegression,Lasso, Ridge,LogisticRegressionCV,RidgeCV,LassoCV,ElasticNetCV,OrthogonalMatchingPursuit,ElasticNet,LassoLarsCV,BayesianRidge\n",
    "from sklearn.tree import DecisionTreeClassifier,DecisionTreeRegressor\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier,KNeighborsRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC,SVR\n",
    "from scipy import stats\n",
    "from scipy.stats import norm, skew\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "\n",
    "\n",
    "from category_encoders.ordinal import OrdinalEncoder\n",
    "from category_encoders.woe import WOEEncoder\n",
    "from category_encoders.target_encoder import TargetEncoder\n",
    "from category_encoders.sum_coding import SumEncoder\n",
    "from category_encoders.m_estimate import MEstimateEncoder\n",
    "from category_encoders.leave_one_out import LeaveOneOutEncoder\n",
    "from category_encoders.helmert import HelmertEncoder\n",
    "from category_encoders.cat_boost import CatBoostEncoder\n",
    "from category_encoders.james_stein import JamesSteinEncoder\n",
    "from category_encoders.one_hot import OneHotEncoder\n",
    "from scipy.special import boxcox1p\n",
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv(r\"C:\\Users\\Prashant\\Desktop\\MU\\Sem 7\\AI in Industry 4.0\\Scm\\Traindata.csv\")\n",
    "test=pd.read_csv(r\"C:\\Users\\Prashant\\Desktop\\MU\\Sem 7\\AI in Industry 4.0\\Scm\\DataSetRetail.csv\")\n",
    "submission=pd.read_csv(r\"C:\\Users\\Prashant\\Desktop\\MU\\Sem 7\\AI in Industry 4.0\\Scm\\submission format.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imputing missing value with the relevant total price\n",
    "train.total_price=train.total_price.fillna(469.5375)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.isna().sum().sum())\n",
    "print(test.isna().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#New Feature Creation functions\n",
    "\n",
    "def gen_count_id(train,test,col,name):\n",
    "    temp=train.groupby(col)['record_ID'].count().reset_index().rename(columns={'record_ID':name})\n",
    "    train=pd.merge(train,temp,how='left',on=col)\n",
    "    test=pd.merge(test,temp,how='left',on=col)\n",
    "    train[name]=train[name].astype(float)\n",
    "    test[name]=test[name].astype(float)\n",
    "    train[name].fillna(np.median(temp[name]),inplace=True)\n",
    "    test[name].fillna(np.median(temp[name]),inplace=True)\n",
    "    return train,test\n",
    "\n",
    "def gen_average_units(train,test,col,name):\n",
    "    temp=train.groupby(col)['units_sold'].mean().reset_index().rename(columns={'units_sold':name})\n",
    "    train=pd.merge(train,temp,how='left',on=col)\n",
    "    test=pd.merge(test,temp,how='left',on=col)\n",
    "    train[name].fillna(np.median(temp[name]),inplace=True)\n",
    "    test[name].fillna(np.median(temp[name]),inplace=True)\n",
    "    return train,test\n",
    "\n",
    "def gen_average_price(train,test,col,price='base_price',name='name'):\n",
    "    temp=train.groupby(col)[price].mean().reset_index().rename(columns={price:name})\n",
    "    train=pd.merge(train,temp,how='left',on=col)\n",
    "    test=pd.merge(test,temp,how='left',on=col)\n",
    "    train[name].fillna(np.median(temp[name]),inplace=True)\n",
    "    test[name].fillna(np.median(temp[name]),inplace=True)\n",
    "    return train,test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test = gen_count_id(train,test,col=['sku_id','store_id'],name='count_id_sku_store') #Genearting count of records per 'sku-id & store-id' \n",
    "train,test = gen_count_id(train,test,col=['sku_id'],name='count_id_sku') #Genearting count of records per 'sku-id'\n",
    "train,test = gen_count_id(train,test,col=['store_id'],name='count_id_store') #Genearting count of records per 'store-id'\n",
    "\n",
    "train,test = gen_average_units(train,test,col=['sku_id','store_id'],name='count_sku_store_id') #Genearting average units sold per 'sku-id & store-id'\n",
    "train,test = gen_average_units(train,test,col=['store_id'],name='count_store_id') #Genearting average units sold per 'store-id'\n",
    "train,test = gen_average_units(train,test,col=['sku_id'],name='count_sku_id') #Genearting average units sold per 'sku-id'\n",
    "\n",
    "train,test = gen_average_price(train,test,col=['sku_id','store_id'],price='base_price',name='price_sku_store') #Genearting average base price per 'sku-id & store-id'\n",
    "train,test = gen_average_price(train,test,col=['sku_id','store_id'],price='total_price',name='price_to_sku_store') #Genearting average total price per 'sku-id & store-id'\n",
    "train,test = gen_average_price(train,test,col=['store_id'],price='base_price',name='price_store_id') #Genearting average base price per 'store-id'\n",
    "train,test = gen_average_price(train,test,col=['sku_id'],price='base_price',name='price_sku_id') #Genearting average base price per 'sku-id'\n",
    "train,test = gen_average_price(train,test,col=['store_id'],price='total_price',name='price_to_store_id') #Genearting average total price per 'store-id'\n",
    "train,test = gen_average_price(train,test,col=['sku_id'],price='total_price',name='price_to_sku_id') #Genearting average total price per 'sku-id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting week feature\n",
    "le = OrdinalEncoder()\n",
    "train['week_1']=le.fit_transform(train['week'])\n",
    "le = OrdinalEncoder()\n",
    "test['week_1']=le.fit_transform(test['week'])+130\n",
    "\n",
    "#Creating week number feature\n",
    "train['week_num']=train.week_1%52\n",
    "test['week_num']=test.week_1%52\n",
    "\n",
    "train['week_num1']=train.week_1%4\n",
    "test['week_num1']=test.week_1%4\n",
    "\n",
    "# Encoding 'week' it using sine and cosine transform; considering it as a cyclic feature \n",
    "train['week_sin'] = np.sin(2 * np.pi * train['week_1'] / 52.143)\n",
    "train['week_cos'] = np.cos(2 * np.pi * train['week_1'] / 52.143)\n",
    "test['week_sin'] = np.sin(2 * np.pi * test['week_1'] / 52.143)\n",
    "test['week_cos'] = np.cos(2 * np.pi * test['week_1'] / 52.143)\n",
    "\n",
    "#Creating feature: percent difference between base price and checkout price.\n",
    "train['price_diff_percent'] = (train['base_price'] - train['total_price']) / train['base_price']\n",
    "test['price_diff_percent'] = (test['base_price'] - test['total_price']) / test['base_price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=train[list(set(train.columns)-set(['record_ID','units_sold','week']))]\n",
    "Y= np.log1p(train['units_sold'])\n",
    "X_test=test[list(set(test.columns)-set(['record_ID','week']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['sku_id'] = X['sku_id'].astype('category')\n",
    "X['store_id'] = X['store_id'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X_test.columns))\n",
    "print(len(X.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test.isna().sum().sum())\n",
    "print(X.isna().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_list=['store_id','sku_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_final=MEstimateEncoder()\n",
    "encoder_final.fit(X[category_list], Y)\n",
    "\n",
    "cat_enc = encoder_final.transform(X[category_list], Y)\n",
    "continuous_train = X.drop(columns= category_list)\n",
    "X = pd.concat([cat_enc,continuous_train],axis=1)\n",
    "\n",
    "test_enc=encoder_final.transform(X_test[category_list])\n",
    "continuous_test=X_test.drop(columns= category_list)\n",
    "X_test=pd.concat([test_enc,continuous_test],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X['week_num1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_valid, y_train, y_valid = train_test_split(X, Y, test_size = 0.2,random_state=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_base = RandomForestRegressor()\n",
    "rf_base.fit(x_train,y_train)\n",
    "\n",
    "\n",
    "rf_tuned = RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=30,\n",
    "                      max_features='sqrt', max_leaf_nodes=None,\n",
    "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "                      min_samples_leaf=1, min_samples_split=10,\n",
    "                      min_weight_fraction_leaf=0.0, n_estimators=600,\n",
    "                      n_jobs=None, oob_score=True, random_state=None,\n",
    "                      verbose=0, warm_start=False)\n",
    "rf_tuned.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lgb_base=lgb.LGBMRegressor(objective='regression')\n",
    "model_lgb_base.fit(x_train,y_train)\n",
    "\n",
    "model_lgb_tuned=lgb.LGBMRegressor(bagging_fraction=0.8, bagging_frequency=4, boosting_type='gbdt',\n",
    "              class_weight=None, colsample_bytree=1.0, feature_fraction=0.5,\n",
    "              importance_type='split', learning_rate=0.1, max_depth=30,\n",
    "              min_child_samples=20, min_child_weight=30, min_data_in_leaf=70,\n",
    "              min_split_gain=0.0001, n_estimators=200, n_jobs=-1,\n",
    "              num_leaves=1200, objective=None, random_state=None, reg_alpha=0.0,\n",
    "              reg_lambda=0.0, silent=True, subsample=1.0,\n",
    "              subsample_for_bin=200000, subsample_freq=0)\n",
    "\n",
    "model_lgb_tuned.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_rfb_valid=rf_base.predict(x_valid)\n",
    "prediction_rft_valid=rf_tuned.predict(x_valid)\n",
    "prediction_lgbmb_valid=model_lgb_base.predict(x_valid)\n",
    "prediction_lgbmt_valid=model_lgb_tuned.predict(x_valid)\n",
    "\n",
    "rf_base_msle=100*mean_squared_log_error(y_valid,prediction_rfb_valid)\n",
    "rf_tuned_msle=100*mean_squared_log_error(y_valid,prediction_rft_valid)\n",
    "lgbm_base_msle=100*mean_squared_log_error(y_valid,prediction_lgbmb_valid)\n",
    "lgbm_tuned_msle=100*mean_squared_log_error(y_valid,prediction_lgbmt_valid)\n",
    "\n",
    "prediction_ensemble_base=(((1-rf_base_msle)*prediction_rfb_valid)+((1-lgbm_base_msle)*prediction_lgbmb_valid))/(2-rf_base_msle-lgbm_base_msle)\n",
    "prediction_ensemble_tuned=(((1-rf_tuned_msle)*prediction_rft_valid)+((1-lgbm_tuned_msle)*prediction_lgbmt_valid))/(2-rf_tuned_msle-lgbm_tuned_msle)\n",
    "\n",
    "ensemble_base_msle=100*mean_squared_log_error(y_valid,prediction_ensemble_base)\n",
    "ensemble_tuned_msle=100*mean_squared_log_error(y_valid,prediction_ensemble_tuned)\n",
    "\n",
    "\n",
    "print(\"RF Base: {}; RF Tuned: {}\".format(rf_base_msle,rf_tuned_msle))\n",
    "print(\"LGBM Base: {}; LGBM Tuned: {}\".format(lgbm_base_msle,lgbm_tuned_msle))\n",
    "print(\"Ensemble Base: {}; Ensemble Tuned: {}\".format(ensemble_base_msle,ensemble_tuned_msle))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lgb.LGBMRegressor(bagging_fraction=0.8, bagging_frequency=4, boosting_type='gbdt',\n",
    "              class_weight=None, colsample_bytree=1.0, feature_fraction=0.5,\n",
    "              importance_type='split', learning_rate=0.1, max_depth=30,\n",
    "              min_child_samples=20, min_child_weight=30, min_data_in_leaf=70,\n",
    "              min_split_gain=0.0001, n_estimators=100, n_jobs=-1,\n",
    "              num_leaves=1400, objective=None, random_state=None, reg_alpha=0.0,\n",
    "              reg_lambda=0.0, silent=True, subsample=1.0,\n",
    "              subsample_for_bin=200000, subsample_freq=0)\n",
    "\n",
    "model.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_test['week_num1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction=model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_prediction=np.round(np.expm1(prediction))\n",
    "submission['units_sold']=final_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('output.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
